{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 09: Advanced Differential Expression Analysis\n",
    "\n",
    "This tutorial covers advanced statistical methods for differential expression (DE) analysis in single-cell proteomics data. We explore methods designed for specific data characteristics:\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand when to use count-based models vs non-parametric methods\n",
    "- Apply VOOM + limma analysis for count data\n",
    "- Use limma-trend for mean-variance dependency\n",
    "- Perform DESeq2-like negative binomial analysis\n",
    "- Apply Wilcoxon rank-sum and Brunner-Munzel tests\n",
    "- Handle paired samples with non-parametric tests\n",
    "- Compare and validate results across multiple methods\n",
    "\n",
    "---\n",
    "\n",
    "## Methods Covered\n",
    "\n",
    "### Count-Based Models:\n",
    "- **VOOM**: Precision weights for RNA-seq like data\n",
    "- **limma-trend**: Empirical Bayes with trend correction\n",
    "- **DESeq2**: Negative binomial GLM\n",
    "\n",
    "### Non-Parametric Methods:\n",
    "- **Wilcoxon**: Rank-sum test (paired/unpaired)\n",
    "- **Brunner-Munzel**: Heteroscedastic robust test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import required libraries and configure the plotting environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# Apply SciencePlots style for publication-quality figures\n",
    "plt.style.use([\"science\", \"no-latex\"])\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# Import ScpTensor\n",
    "import scptensor\n",
    "from scptensor import (\n",
    "    create_test_container,\n",
    ")\n",
    "from scptensor.diff_expr import (\n",
    "    diff_expr_brunner_munzel,\n",
    "    diff_expr_deseq2,\n",
    "    diff_expr_limma_trend,\n",
    "    diff_expr_voom,\n",
    "    diff_expr_wilcoxon,\n",
    ")\n",
    "\n",
    "print(f\"ScpTensor version: {scptensor.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Method Selection Guide\n",
    "\n",
    "Choosing the right statistical method is crucial for valid results:\n",
    "\n",
    "| Method | Data Type | Key Assumption | Best For |\n",
    "|--------|-----------|----------------|----------|\n",
    "| **VOOM** | Counts | Mean-variance trend | Small samples, RNA-seq like |\n",
    "| **limma-trend** | Counts | Mean-variance dependency | Trended variance |\n",
    "| **DESeq2** | Counts | Negative binomial | Over-dispersed counts |\n",
    "| **Wilcoxon** | Any | None (non-parametric) | Non-normal data |\n",
    "| **Brunner-Munzel** | Any | None (non-parametric) | Unequal variances |\n",
    "\n",
    "### Quick Decision Flow:\n",
    "1. **Count data with many samples (>10 per group)**: DESeq2\n",
    "2. **Count data with few samples**: VOOM or limma-trend\n",
    "3. **Continuous/normalized data**: Wilcoxon or Brunner-Munzel\n",
    "4. **Unequal variances**: Brunner-Munzel > Wilcoxon\n",
    "5. **Paired samples**: Wilcoxon with `paired=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Example Dataset\n",
    "\n",
    "For this tutorial, we'll create a simulated dataset with count-like characteristics that demonstrates the differences between methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simulated single-cell proteomics data\n",
    "# with two groups and differential expression\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_samples_a = 20\n",
    "n_samples_b = 20\n",
    "n_features = 200\n",
    "\n",
    "# Generate count-like data (negative binomial distribution)\n",
    "# Group A: baseline expression\n",
    "counts_a = np.random.negative_binomial(10, 0.5, size=(n_samples_a, n_features))\n",
    "\n",
    "# Group B: differential expression for some features\n",
    "counts_b = np.random.negative_binomial(10, 0.5, size=(n_samples_b, n_features))\n",
    "\n",
    "# Add differential expression:\n",
    "# First 20 features: up-regulated in group A\n",
    "# Features 20-40: down-regulated in group A\n",
    "# Features 40-60: slightly up-regulated\n",
    "# Remaining: no change\n",
    "counts_a[:, :20] = np.random.negative_binomial(20, 0.4, size=(n_samples_a, 20))  # Up in A\n",
    "counts_b[:, :20] = np.random.negative_binomial(5, 0.6, size=(n_samples_b, 20))  # Down in B\n",
    "\n",
    "counts_a[:, 20:40] = np.random.negative_binomial(5, 0.6, size=(n_samples_a, 20))  # Down in A\n",
    "counts_b[:, 20:40] = np.random.negative_binomial(20, 0.4, size=(n_samples_b, 20))  # Up in B\n",
    "\n",
    "# Combine data\n",
    "X_counts = np.vstack([counts_a, counts_b])\n",
    "\n",
    "# Create metadata\n",
    "sample_ids = [f\"sample_{i}\" for i in range(n_samples_a + n_samples_b)]\n",
    "groups = [\"A\"] * n_samples_a + [\"B\"] * n_samples_b\n",
    "batches = np.random.choice([\"batch1\", \"batch2\"], size=n_samples_a + n_samples_b)\n",
    "\n",
    "# Create feature names\n",
    "feature_ids = [f\"protein_{i}\" for i in range(n_features)]\n",
    "\n",
    "# Create container\n",
    "container = create_test_container(\n",
    "    n_samples=n_samples_a + n_samples_b,\n",
    "    n_features=n_features,\n",
    "    sparse=False,\n",
    ")\n",
    "\n",
    "# Replace with our count data\n",
    "container.assays[\"proteins\"].layers[\"raw\"].X = X_counts\n",
    "container.obs = container.obs.with_columns(\n",
    "    [\n",
    "        pl.Series(\"_index\", sample_ids),\n",
    "        pl.Series(\"group\", groups),\n",
    "        pl.Series(\"batch\", batches),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Update feature IDs\n",
    "container.assays[\"proteins\"].var = container.assays[\"proteins\"].var.with_columns(\n",
    "    [pl.Series(\"feature_id\", feature_ids)]\n",
    ")\n",
    "container.assays[\"proteins\"].feature_id_col = \"feature_id\"\n",
    "\n",
    "print(\"Dataset created:\")\n",
    "print(f\"  Samples: {container.n_samples}\")\n",
    "print(f\"  Features: {container.assays['proteins'].n_features}\")\n",
    "print(f\"  Group A: {n_samples_a} samples\")\n",
    "print(f\"  Group B: {n_samples_b} samples\")\n",
    "print(\"\\nGroup distribution:\")\n",
    "print(container.obs.group_by(\"group\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Explore Data Characteristics\n",
    "\n",
    "Before choosing a method, let's examine the data properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize mean-variance relationship\n",
    "means = np.mean(X_counts, axis=0)\n",
    "variances = np.var(X_counts, axis=0, ddof=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Mean-variance plot\n",
    "axes[0].scatter(means, variances, alpha=0.5, s=20)\n",
    "axes[0].set_xlabel(\"Mean Expression\")\n",
    "axes[0].set_ylabel(\"Variance\")\n",
    "axes[0].set_title(\"Mean-Variance Relationship\")\n",
    "axes[0].loglog()\n",
    "\n",
    "# Add theoretical lines\n",
    "x_line = np.linspace(min(means), max(means), 100)\n",
    "axes[0].plot(x_line, x_line, \"r--\", label=\"Poisson (var = mean)\", alpha=0.7)\n",
    "axes[0].plot(x_line, x_line**1.5, \"g--\", label=\"Over-dispersed\", alpha=0.7)\n",
    "axes[0].legend()\n",
    "\n",
    "# Count distribution histogram\n",
    "axes[1].hist(X_counts.flatten(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].set_xlabel(\"Count Value\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].set_title(\"Count Distribution\")\n",
    "axes[1].set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/data_characteristics.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Data characteristics:\")\n",
    "print(\n",
    "    f\"  Mean-variance correlation: {np.corrcoef(np.log10(means + 1), np.log10(variances + 1))[0, 1]:.3f}\"\n",
    ")\n",
    "print(f\"  Sparsity: {(X_counts == 0).sum() / X_counts.size * 100:.1f}% zeros\")\n",
    "print(f\"  Median count: {np.median(X_counts):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Count-Based Models\n",
    "\n",
    "Count-based models are designed for data where:\n",
    "- Values are non-negative integers (counts)\n",
    "- Variance depends on mean (heteroscedastic)\n",
    "- Distribution may be over-dispersed\n",
    "\n",
    "### 4.1 VOOM + limma Analysis\n",
    "\n",
    "VOOM (mean-variance modelling at the observational level) transforms counts to log2-CPM with precision weights, then applies limma's empirical Bayes moderation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run VOOM analysis\n",
    "result_voom = diff_expr_voom(\n",
    "    container=container,\n",
    "    assay_name=\"proteins\",\n",
    "    layer=\"raw\",\n",
    "    groupby=\"group\",\n",
    "    group1=\"A\",\n",
    "    group2=\"B\",\n",
    "    min_count=10,\n",
    "    normalize=\"tmm\",\n",
    ")\n",
    "\n",
    "print(\"VOOM Analysis Results:\")\n",
    "print(f\"  Method: {result_voom.method}\")\n",
    "print(f\"  Features tested: {np.sum(~np.isnan(result_voom.p_values))}\")\n",
    "print(f\"  Significant (FDR < 0.05): {np.sum(result_voom.p_values_adj < 0.05)}\")\n",
    "print(\n",
    "    f\"  Up-regulated in A: {np.sum((result_voom.p_values_adj < 0.05) & (result_voom.log2_fc > 1))}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Down-regulated in A: {np.sum((result_voom.p_values_adj < 0.05) & (result_voom.log2_fc < -1))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 limma-trend Analysis\n",
    "\n",
    "limma-trend applies empirical Bayes variance shrinkage with trend correction for the mean-variance relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run limma-trend analysis\n",
    "result_trend = diff_expr_limma_trend(\n",
    "    container=container,\n",
    "    assay_name=\"proteins\",\n",
    "    layer=\"raw\",\n",
    "    groupby=\"group\",\n",
    "    group1=\"A\",\n",
    "    group2=\"B\",\n",
    "    trend=True,\n",
    "    robust=True,\n",
    ")\n",
    "\n",
    "print(\"limma-trend Analysis Results:\")\n",
    "print(f\"  Method: {result_trend.method}\")\n",
    "print(f\"  Features tested: {np.sum(~np.isnan(result_trend.p_values))}\")\n",
    "print(f\"  Significant (FDR < 0.05): {np.sum(result_trend.p_values_adj < 0.05)}\")\n",
    "print(\n",
    "    f\"  Up-regulated in A: {np.sum((result_trend.p_values_adj < 0.05) & (result_trend.log2_fc > 1))}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Down-regulated in A: {np.sum((result_trend.p_values_adj < 0.05) & (result_trend.log2_fc < -1))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 DESeq2-like Analysis\n",
    "\n",
    "DESeq2 uses a negative binomial generalized linear model, accounting for overdispersion common in count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DESeq2-like analysis\n",
    "result_deseq2 = diff_expr_deseq2(\n",
    "    container=container,\n",
    "    assay_name=\"proteins\",\n",
    "    layer=\"raw\",\n",
    "    groupby=\"group\",\n",
    "    group1=\"A\",\n",
    "    group2=\"B\",\n",
    "    fit_type=\"parametric\",\n",
    "    test=\"wald\",\n",
    "    min_count=10,\n",
    ")\n",
    "\n",
    "print(\"DESeq2 Analysis Results:\")\n",
    "print(f\"  Method: {result_deseq2.method}\")\n",
    "print(f\"  Features tested: {np.sum(~np.isnan(result_deseq2.p_values))}\")\n",
    "print(f\"  Significant (FDR < 0.05): {np.sum(result_deseq2.p_values_adj < 0.05)}\")\n",
    "print(\n",
    "    f\"  Up-regulated in A: {np.sum((result_deseq2.p_values_adj < 0.05) & (result_deseq2.log2_fc > 1))}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Down-regulated in A: {np.sum((result_deseq2.p_values_adj < 0.05) & (result_deseq2.log2_fc < -1))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Comparing Count-Based Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results across methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# P-value comparison\n",
    "valid = ~np.isnan(result_voom.p_values_adj) & ~np.isnan(result_deseq2.p_values_adj)\n",
    "axes[0, 0].scatter(\n",
    "    result_voom.p_values_adj[valid], result_deseq2.p_values_adj[valid], alpha=0.5, s=15\n",
    ")\n",
    "axes[0, 0].plot([0, 1], [0, 1], \"r--\", linewidth=1)\n",
    "axes[0, 0].set_xlabel(\"VOOM Adjusted P-value\")\n",
    "axes[0, 0].set_ylabel(\"DESeq2 Adjusted P-value\")\n",
    "axes[0, 0].set_title(\"VOOM vs DESeq2 P-values\")\n",
    "\n",
    "# Log2 FC comparison\n",
    "axes[0, 1].scatter(result_voom.log2_fc, result_deseq2.log2_fc, alpha=0.5, s=15)\n",
    "min_fc = min(result_voom.log2_fc.min(), result_deseq2.log2_fc.min())\n",
    "max_fc = max(result_voom.log2_fc.max(), result_deseq2.log2_fc.max())\n",
    "axes[0, 1].plot([min_fc, max_fc], [min_fc, max_fc], \"r--\", linewidth=1)\n",
    "axes[0, 1].set_xlabel(\"VOOM log2 FC\")\n",
    "axes[0, 1].set_ylabel(\"DESeq2 log2 FC\")\n",
    "axes[0, 1].set_title(\"VOOM vs DESeq2 Log2 Fold Change\")\n",
    "\n",
    "# Venn diagram of significant features\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "sig_voom = set(np.where(result_voom.p_values_adj < 0.05)[0])\n",
    "sig_trend = set(np.where(result_trend.p_values_adj < 0.05)[0])\n",
    "sig_deseq2 = set(np.where(result_deseq2.p_values_adj < 0.05)[0])\n",
    "\n",
    "venn3([sig_voom, sig_trend, sig_deseq2], (\"VOOM\", \"limma-trend\", \"DESeq2\"), ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Overlap of Significant Features (FDR < 0.05)\")\n",
    "\n",
    "# Method concordance\n",
    "methods = [\"VOOM\", \"limma-trend\", \"DESeq2\"]\n",
    "n_sig = [len(sig_voom), len(sig_trend), len(sig_deseq2)]\n",
    "bars = axes[1, 1].bar(methods, n_sig, color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n",
    "axes[1, 1].set_ylabel(\"Number of Significant Features\")\n",
    "axes[1, 1].set_title(\"Significant Features by Method\")\n",
    "axes[1, 1].set_ylim(0, max(n_sig) * 1.2)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, n_sig, strict=False):\n",
    "    axes[1, 1].text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 1,\n",
    "        str(count),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/count_methods_comparison.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConcordance Analysis:\")\n",
    "print(f\"  VOOM & limma-trend overlap: {len(sig_voom & sig_trend)} features\")\n",
    "print(f\"  VOOM & DESeq2 overlap: {len(sig_voom & sig_deseq2)} features\")\n",
    "print(f\"  All three methods: {len(sig_voom & sig_trend & sig_deseq2)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Non-Parametric Methods\n",
    "\n",
    "Non-parametric methods make no distributional assumptions and are robust to:\n",
    "- Non-normal distributions\n",
    "- Outliers\n",
    "- Unequal variances\n",
    "\n",
    "### 5.1 Wilcoxon Rank-Sum Test\n",
    "\n",
    "The Wilcoxon rank-sum test (Mann-Whitney U) tests whether samples from one group tend to have higher values than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Wilcoxon rank-sum test\n",
    "result_wilcoxon = diff_expr_wilcoxon(\n",
    "    container=container,\n",
    "    assay_name=\"proteins\",\n",
    "    layer=\"raw\",\n",
    "    groupby=\"group\",\n",
    "    group1=\"A\",\n",
    "    group2=\"B\",\n",
    "    paired=False,\n",
    "    alternative=\"two-sided\",\n",
    "    min_samples_per_group=3,\n",
    "    missing_strategy=\"ignore\",\n",
    ")\n",
    "\n",
    "print(\"Wilcoxon Rank-Sum Test Results:\")\n",
    "print(f\"  Method: {result_wilcoxon.method}\")\n",
    "print(f\"  Features tested: {np.sum(~np.isnan(result_wilcoxon.p_values))}\")\n",
    "print(f\"  Significant (FDR < 0.05): {np.sum(result_wilcoxon.p_values_adj < 0.05)}\")\n",
    "print(\n",
    "    f\"  Up-regulated in A: {np.sum((result_wilcoxon.p_values_adj < 0.05) & (result_wilcoxon.log2_fc > 1))}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Down-regulated in A: {np.sum((result_wilcoxon.p_values_adj < 0.05) & (result_wilcoxon.log2_fc < -1))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Brunner-Munzel Test\n",
    "\n",
    "The Brunner-Munzel test is robust to unequal variances (heteroscedasticity) and handles unequal sample sizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Brunner-Munzel test\n",
    "result_bm = diff_expr_brunner_munzel(\n",
    "    container=container,\n",
    "    assay_name=\"proteins\",\n",
    "    layer=\"raw\",\n",
    "    groupby=\"group\",\n",
    "    group1=\"A\",\n",
    "    group2=\"B\",\n",
    "    alternative=\"two-sided\",\n",
    "    min_samples_per_group=3,\n",
    "    missing_strategy=\"ignore\",\n",
    ")\n",
    "\n",
    "print(\"Brunner-Munzel Test Results:\")\n",
    "print(f\"  Method: {result_bm.method}\")\n",
    "print(f\"  Features tested: {np.sum(~np.isnan(result_bm.p_values))}\")\n",
    "print(f\"  Significant (FDR < 0.05): {np.sum(result_bm.p_values_adj < 0.05)}\")\n",
    "print(f\"  Up-regulated in A: {np.sum((result_bm.p_values_adj < 0.05) & (result_bm.log2_fc > 1))}\")\n",
    "print(\n",
    "    f\"  Down-regulated in A: {np.sum((result_bm.p_values_adj < 0.05) & (result_bm.log2_fc < -1))}\"\n",
    ")\n",
    "\n",
    "# Interpret relative effects\n",
    "print(\"\\nRelative Effects (pHat):\")\n",
    "p_hat = result_bm.effect_sizes\n",
    "print(f\"  Mean pHat: {np.nanmean(p_hat):.3f}\")\n",
    "print(\"  pHat = 0.5: Stochastic equality (no difference)\")\n",
    "print(\"  pHat > 0.5: Group A tends to have larger values\")\n",
    "print(\"  pHat < 0.5: Group B tends to have larger values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Comparing Non-Parametric Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Wilcoxon and Brunner-Munzel\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# P-value comparison\n",
    "valid_np = ~np.isnan(result_wilcoxon.p_values_adj) & ~np.isnan(result_bm.p_values_adj)\n",
    "axes[0].scatter(\n",
    "    result_wilcoxon.p_values_adj[valid_np], result_bm.p_values_adj[valid_np], alpha=0.5, s=15\n",
    ")\n",
    "axes[0].plot([0, 1], [0, 1], \"r--\", linewidth=1)\n",
    "axes[0].set_xlabel(\"Wilcoxon Adjusted P-value\")\n",
    "axes[0].set_ylabel(\"Brunner-Munzel Adjusted P-value\")\n",
    "axes[0].set_title(\"Wilcoxon vs Brunner-Munzel P-values\")\n",
    "\n",
    "# Log2 FC comparison\n",
    "axes[1].scatter(result_wilcoxon.log2_fc, result_bm.log2_fc, alpha=0.5, s=15)\n",
    "min_fc = min(result_wilcoxon.log2_fc.min(), result_bm.log2_fc.min())\n",
    "max_fc = max(result_wilcoxon.log2_fc.max(), result_bm.log2_fc.max())\n",
    "axes[1].plot([min_fc, max_fc], [min_fc, max_fc], \"r--\", linewidth=1)\n",
    "axes[1].set_xlabel(\"Wilcoxon log2 FC\")\n",
    "axes[1].set_ylabel(\"Brunner-Munzel log2 FC\")\n",
    "axes[1].set_title(\"Wilcoxon vs Brunner-Munzel Log2 Fold Change\")\n",
    "\n",
    "# Significant feature comparison\n",
    "sig_wilcoxon = set(np.where(result_wilcoxon.p_values_adj < 0.05)[0])\n",
    "sig_bm = set(np.where(result_bm.p_values_adj < 0.05)[0])\n",
    "\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "venn2([sig_wilcoxon, sig_bm], (\"Wilcoxon\", \"Brunner-Munzel\"), ax=axes[2])\n",
    "axes[2].set_title(\"Overlap of Significant Features (FDR < 0.05)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/nonparametric_comparison.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNon-Parametric Method Concordance:\")\n",
    "print(f\"  Wilcoxon only: {len(sig_wilcoxon - sig_bm)} features\")\n",
    "print(f\"  Brunner-Munzel only: {len(sig_bm - sig_wilcoxon)} features\")\n",
    "print(f\"  Both methods: {len(sig_wilcoxon & sig_bm)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Paired Sample Analysis\n",
    "\n",
    "For paired/matched samples (e.g., before-after treatment), use the paired Wilcoxon test. Let's create a paired dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paired dataset\n",
    "n_pairs = 15\n",
    "\n",
    "# Simulate matched pairs (e.g., same patient before/after treatment)\n",
    "# Each pair has one control and one treatment sample\n",
    "paired_counts_control = np.random.negative_binomial(15, 0.5, size=(n_pairs, n_features))\n",
    "paired_counts_treatment = paired_counts_control.copy()\n",
    "\n",
    "# Add treatment effect to some features\n",
    "paired_counts_treatment[:, :15] += np.random.poisson(10, size=(n_pairs, 15))\n",
    "paired_counts_treatment[:, 15:30] -= np.random.poisson(5, size=(n_pairs, 15))\n",
    "paired_counts_treatment = np.maximum(paired_counts_treatment, 0)\n",
    "\n",
    "# Combine data\n",
    "paired_X = np.vstack([paired_counts_control, paired_counts_treatment])\n",
    "\n",
    "# Create metadata with pair IDs\n",
    "paired_sample_ids = []\n",
    "paired_groups = []\n",
    "pair_ids = []\n",
    "\n",
    "for i in range(n_pairs):\n",
    "    paired_sample_ids.append(f\"pair_{i}_control\")\n",
    "    paired_groups.append(\"control\")\n",
    "    pair_ids.append(f\"pair_{i}\")\n",
    "\n",
    "    paired_sample_ids.append(f\"pair_{i}_treatment\")\n",
    "    paired_groups.append(\"treatment\")\n",
    "    pair_ids.append(f\"pair_{i}\")\n",
    "\n",
    "# Create paired container\n",
    "paired_container = create_test_container(\n",
    "    n_samples=n_pairs * 2,\n",
    "    n_features=n_features,\n",
    "    sparse=False,\n",
    ")\n",
    "\n",
    "paired_container.assays[\"proteins\"].layers[\"raw\"].X = paired_X\n",
    "paired_container.obs = paired_container.obs.with_columns(\n",
    "    [\n",
    "        pl.Series(\"_index\", paired_sample_ids),\n",
    "        pl.Series(\"group\", paired_groups),\n",
    "        pl.Series(\"pair_id\", pair_ids),\n",
    "    ]\n",
    ")\n",
    "\n",
    "paired_container.assays[\"proteins\"].var = paired_container.assays[\"proteins\"].var.with_columns(\n",
    "    [pl.Series(\"feature_id\", feature_ids)]\n",
    ")\n",
    "paired_container.assays[\"proteins\"].feature_id_col = \"feature_id\"\n",
    "\n",
    "print(\"Paired dataset created:\")\n",
    "print(f\"  Total samples: {paired_container.n_samples}\")\n",
    "print(f\"  Number of pairs: {n_pairs}\")\n",
    "print(f\"  Control: {n_pairs} samples\")\n",
    "print(f\"  Treatment: {n_pairs} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Paired Wilcoxon Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run paired Wilcoxon test\n",
    "result_paired = diff_expr_wilcoxon(\n",
    "    container=paired_container,\n",
    "    assay_name=\"proteins\",\n",
    "    layer=\"raw\",\n",
    "    groupby=\"group\",\n",
    "    group1=\"treatment\",\n",
    "    group2=\"control\",\n",
    "    paired=True,\n",
    "    pair_id_col=\"pair_id\",\n",
    "    alternative=\"two-sided\",\n",
    "    min_samples_per_group=3,\n",
    "    missing_strategy=\"ignore\",\n",
    ")\n",
    "\n",
    "print(\"Paired Wilcoxon Test Results:\")\n",
    "print(f\"  Method: {result_paired.method}\")\n",
    "print(f\"  Number of pairs analyzed: {result_paired.params.get('n_pairs', 'N/A')}\")\n",
    "print(f\"  Features tested: {np.sum(~np.isnan(result_paired.p_values))}\")\n",
    "print(f\"  Significant (FDR < 0.05): {np.sum(result_paired.p_values_adj < 0.05)}\")\n",
    "print(\n",
    "    f\"  Up in treatment: {np.sum((result_paired.p_values_adj < 0.05) & (result_paired.log2_fc > 1))}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Down in treatment: {np.sum((result_paired.p_values_adj < 0.05) & (result_paired.log2_fc < -1))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Paired vs Unpaired Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare paired vs unpaired analysis\n",
    "result_unpaired = diff_expr_wilcoxon(\n",
    "    container=paired_container,\n",
    "    assay_name=\"proteins\",\n",
    "    layer=\"raw\",\n",
    "    groupby=\"group\",\n",
    "    group1=\"treatment\",\n",
    "    group2=\"control\",\n",
    "    paired=False,\n",
    "    alternative=\"two-sided\",\n",
    "    missing_strategy=\"ignore\",\n",
    ")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# P-value comparison\n",
    "valid_pair = ~np.isnan(result_paired.p_values_adj) & ~np.isnan(result_unpaired.p_values_adj)\n",
    "axes[0].scatter(\n",
    "    result_unpaired.p_values_adj[valid_pair],\n",
    "    result_paired.p_values_adj[valid_pair],\n",
    "    alpha=0.5,\n",
    "    s=15,\n",
    ")\n",
    "axes[0].plot([0, 1], [0, 1], \"r--\", linewidth=1)\n",
    "axes[0].set_xlabel(\"Unpaired Adjusted P-value\")\n",
    "axes[0].set_ylabel(\"Paired Adjusted P-value\")\n",
    "axes[0].set_title(\"Paired vs Unpaired Wilcoxon P-values\")\n",
    "\n",
    "# Significant features\n",
    "sig_paired = set(np.where(result_paired.p_values_adj < 0.05)[0])\n",
    "sig_unpaired = set(np.where(result_unpaired.p_values_adj < 0.05)[0])\n",
    "\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "venn2([sig_unpaired, sig_paired], (\"Unpaired\", \"Paired\"), ax=axes[1])\n",
    "axes[1].set_title(\"Significant Features (FDR < 0.05)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/paired_vs_unpaired.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPaired vs Unpaired Analysis:\")\n",
    "print(f\"  Unpaired only: {len(sig_unpaired - sig_paired)} features\")\n",
    "print(f\"  Paired only: {len(sig_paired - sig_unpaired)} features\")\n",
    "print(f\"  Both methods: {len(sig_paired & sig_unpaired)} features\")\n",
    "print(\"\\nNote: Paired analysis typically has more power when pairs are matched.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Result Visualization\n",
    "\n",
    "### 7.1 Volcano Plots for Different Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volcano plots for all methods\n",
    "results = {\n",
    "    \"VOOM\": result_voom,\n",
    "    \"limma-trend\": result_trend,\n",
    "    \"DESeq2\": result_deseq2,\n",
    "    \"Wilcoxon\": result_wilcoxon,\n",
    "    \"Brunner-Munzel\": result_bm,\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "\n",
    "    ax = axes[idx]\n",
    "\n",
    "    log2fc = result.log2_fc\n",
    "    neg_log_p = -np.log10(result.p_values_adj)\n",
    "\n",
    "    # Color points\n",
    "    colors = np.full(len(log2fc), \"gray\", dtype=object)\n",
    "    valid = ~np.isnan(result.p_values_adj)\n",
    "\n",
    "    sig = result.p_values_adj < 0.05\n",
    "    colors[valid & sig & (log2fc > 1)] = \"#d62728\"  # red - up\n",
    "    colors[valid & sig & (log2fc < -1)] = \"#1f77b4\"  # blue - down\n",
    "\n",
    "    ax.scatter(log2fc[valid], neg_log_p[valid], c=colors[valid], alpha=0.6, s=20)\n",
    "\n",
    "    # Threshold lines\n",
    "    ax.axhline(-np.log10(0.05), color=\"black\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.axvline(1, color=\"black\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.axvline(-1, color=\"black\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    ax.set_xlabel(\"log2 Fold Change\")\n",
    "    ax.set_ylabel(\"-log10 Adjusted P-value\")\n",
    "    ax.set_title(f\"{name}\")\n",
    "    ax.set_ylim(0, max(neg_log_p[valid]) * 1.1)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/volcano_plots_all_methods.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Volcano plots generated for all methods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 MA Plot (Intensity vs Fold Change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MA plot for VOOM results\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Calculate M (log2 fold change) and A (average expression)\n",
    "M = result_voom.log2_fc\n",
    "A = (result_voom.group_stats[\"A_mean\"] + result_voom.group_stats[\"B_mean\"]) / 2\n",
    "A = np.log2(A + 1)  # Log transform average expression\n",
    "\n",
    "# Color by significance\n",
    "colors = np.full(len(M), \"gray\", dtype=object)\n",
    "valid = ~np.isnan(result_voom.p_values_adj)\n",
    "sig = result_voom.p_values_adj < 0.05\n",
    "\n",
    "colors[valid & sig & (M > 1)] = \"#d62728\"  # red - up\n",
    "colors[valid & sig & (M < -1)] = \"#1f77b4\"  # blue - down\n",
    "\n",
    "ax.scatter(A[valid], M[valid], c=colors[valid], alpha=0.6, s=20)\n",
    "\n",
    "# Add threshold lines\n",
    "ax.axhline(0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.axhline(1, color=\"black\", linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "ax.axhline(-1, color=\"black\", linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(\"Average Expression (log2)\")\n",
    "ax.set_ylabel(\"log2 Fold Change\")\n",
    "ax.set_title(\"MA Plot: VOOM Analysis\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/ma_plot.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"MA plot generated.\")\n",
    "print(\"The MA plot shows:\")\n",
    "print(\"  X-axis: Average expression across groups\")\n",
    "print(\"  Y-axis: Log2 fold change (M = difference, A = average)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 P-value Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare p-value distributions across methods\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# P-value histogram\n",
    "bins = np.linspace(0, 1, 21)\n",
    "for name, result in results.items():\n",
    "    pvals = result.p_values_adj[~np.isnan(result.p_values_adj)]\n",
    "    axes[0].hist(pvals, bins=bins, alpha=0.5, label=name, density=True)\n",
    "\n",
    "axes[0].axvline(0.05, color=\"red\", linestyle=\"--\", linewidth=1, label=\"FDR = 0.05\")\n",
    "axes[0].set_xlabel(\"Adjusted P-value\")\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "axes[0].set_title(\"P-value Distributions\")\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# Q-Q plot (quantile-quantile) for VOOM\n",
    "\n",
    "pvals = result_voom.p_values[~np.isnan(result_voom.p_values)]\n",
    "observed = -np.log10(sorted(pvals))\n",
    "expected = -np.log10(np.linspace(1 / len(pvals), 1, len(pvals)))\n",
    "\n",
    "axes[1].scatter(expected, observed, alpha=0.5, s=15)\n",
    "axes[1].plot([0, max(expected)], [0, max(expected)], \"r--\", linewidth=1, label=\"Expected (null)\")\n",
    "axes[1].set_xlabel(\"Expected -log10 P-value\")\n",
    "axes[1].set_ylabel(\"Observed -log10 P-value\")\n",
    "axes[1].set_title(\"Q-Q Plot: VOOM Analysis\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/pvalue_distributions.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"P-value distribution analysis complete.\")\n",
    "print(\"A deviation above the diagonal in the Q-Q plot indicates enrichment of low p-values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handling Common Challenges\n",
    "\n",
    "### 8.1 Low Count Filtering\n",
    "\n",
    "The `min_count` parameter controls feature filtering. Features with insufficient counts are excluded from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate effect of min_count filtering\n",
    "min_counts = [0, 5, 10, 20, 50]\n",
    "n_tested = []\n",
    "n_significant = []\n",
    "\n",
    "for mc in min_counts:\n",
    "    result = diff_expr_voom(\n",
    "        container=container,\n",
    "        assay_name=\"proteins\",\n",
    "        layer=\"raw\",\n",
    "        groupby=\"group\",\n",
    "        group1=\"A\",\n",
    "        group2=\"B\",\n",
    "        min_count=mc,\n",
    "        normalize=\"tmm\",\n",
    "    )\n",
    "    n_tested.append(np.sum(~np.isnan(result.p_values)))\n",
    "    n_significant.append(np.sum(result.p_values_adj < 0.05))\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(min_counts))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width / 2, n_tested, width, label=\"Features Tested\", color=\"#1f77b4\")\n",
    "bars2 = ax.bar(\n",
    "    x + width / 2, n_significant, width, label=\"Significant (FDR < 0.05)\", color=\"#d62728\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"min_count Threshold\")\n",
    "ax.set_ylabel(\"Number of Features\")\n",
    "ax.set_title(\"Effect of Low Count Filtering\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(min_counts)\n",
    "ax.legend()\n",
    "\n",
    "# Add count labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"{int(height)}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/mincount_filtering.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEffect of min_count parameter:\")\n",
    "for mc, tested, sig in zip(min_counts, n_tested, n_significant, strict=False):\n",
    "    print(f\"  min_count={mc:3d}: {tested:3d} tested, {sig:3d} significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Handling Missing Values\n",
    "\n",
    "ScpTensor methods natively handle missing values via the mask matrix. The `missing_strategy` parameter controls how NaN values are treated:\n",
    "- **ignore**: Skip missing values (default)\n",
    "- **zero**: Replace with zeros\n",
    "- **median**: Replace with feature median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with missing values for demonstration\n",
    "container_na = container.clone()\n",
    "X_na = container_na.assays[\"proteins\"].layers[\"raw\"].X.copy()\n",
    "\n",
    "# Add random missing values\n",
    "np.random.seed(123)\n",
    "missing_mask = np.random.random(X_na.shape) < 0.1  # 10% missing\n",
    "X_na[missing_mask] = np.nan\n",
    "\n",
    "container_na.assays[\"proteins\"].layers[\"raw\"].X = X_na\n",
    "\n",
    "# Compare missing value strategies\n",
    "strategies = [\"ignore\", \"zero\", \"median\"]\n",
    "results_na = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    try:\n",
    "        result = diff_expr_wilcoxon(\n",
    "            container=container_na,\n",
    "            assay_name=\"proteins\",\n",
    "            layer=\"raw\",\n",
    "            groupby=\"group\",\n",
    "            group1=\"A\",\n",
    "            group2=\"B\",\n",
    "            missing_strategy=strategy,\n",
    "        )\n",
    "        results_na[strategy] = result\n",
    "    except Exception as e:\n",
    "        print(f\"Strategy '{strategy}' failed: {e}\")\n",
    "\n",
    "# Compare results\n",
    "if len(results_na) > 1:\n",
    "    print(\"\\nMissing Value Strategy Comparison:\")\n",
    "    for strategy, result in results_na.items():\n",
    "        n_sig = np.sum(result.p_values_adj < 0.05)\n",
    "        print(f\"  {strategy:8s}: {n_sig} significant features\")\n",
    "\n",
    "print(\"\\nRecommendation: Use 'ignore' (default) for most applications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Method Validation\n",
    "\n",
    "Robust differential expression findings should be consistent across multiple statistical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-method validation\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# Get significant features from each method type\n",
    "sig_count = set(np.where(result_voom.p_values_adj < 0.05)[0])\n",
    "sig_nonpar = set(np.where(result_wilcoxon.p_values_adj < 0.05)[0])\n",
    "sig_brunner = set(np.where(result_bm.p_values_adj < 0.05)[0])\n",
    "\n",
    "# Consensus significant features (found by at least 2 methods)\n",
    "from collections import Counter\n",
    "\n",
    "all_sig = [sig_count, sig_nonpar, sig_brunner]\n",
    "feature_counts = Counter()\n",
    "for sig_set in all_sig:\n",
    "    feature_counts.update(sig_set)\n",
    "\n",
    "consensus = {f for f, c in feature_counts.items() if c >= 2}\n",
    "\n",
    "print(\"Cross-Method Validation:\")\n",
    "print(f\"  Count-based (VOOM): {len(sig_count)} features\")\n",
    "print(f\"  Non-parametric (Wilcoxon): {len(sig_nonpar)} features\")\n",
    "print(f\"  Non-parametric (Brunner-Munzel): {len(sig_brunner)} features\")\n",
    "print(f\"  Consensus (2+ methods): {len(consensus)} features\")\n",
    "\n",
    "# Visualize overlap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Venn diagram\n",
    "venn3(\n",
    "    [sig_count, sig_nonpar, sig_brunner],\n",
    "    (\"Count-based\\n(VOOM)\", \"Non-parametric\\n(Wilcoxon)\", \"Robust\\n(Brunner-Munzel)\"),\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"Method Overlap (FDR < 0.05)\")\n",
    "\n",
    "# Concordance heatmap\n",
    "methods_list = [\"VOOM\", \"Wilcoxon\", \"Brunner-Munzel\"]\n",
    "sig_sets = [sig_count, sig_nonpar, sig_brunner]\n",
    "\n",
    "concordance = np.zeros((3, 3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i == j:\n",
    "            concordance[i, j] = 1.0\n",
    "        else:\n",
    "            concordance[i, j] = len(sig_sets[i] & sig_sets[j]) / len(sig_sets[i] | sig_sets[j])\n",
    "\n",
    "im = axes[1].imshow(concordance, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "axes[1].set_xticks(range(3))\n",
    "axes[1].set_yticks(range(3))\n",
    "axes[1].set_xticklabels(methods_list)\n",
    "axes[1].set_yticklabels(methods_list)\n",
    "axes[1].set_title(\"Method Concordance (Jaccard Index)\")\n",
    "\n",
    "# Add values to heatmap\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        text = axes[1].text(\n",
    "            j, i, f\"{concordance[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\", fontsize=12\n",
    "        )\n",
    "\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/cross_method_validation.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nValidation Strategy:\")\n",
    "print(\"  1. Use consensus features (detected by 2+ methods)\")\n",
    "print(\"  2. Prioritize features with consistent fold change direction\")\n",
    "print(\"  3. Consider biological plausibility of findings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Match method to data characteristics**\n",
    "   - Count data: VOOM, limma-trend, or DESeq2\n",
    "   - Continuous/normalized: Wilcoxon or Brunner-Munzel\n",
    "   - Heteroscedastic: Brunner-Munzel preferred\n",
    "\n",
    "2. **Always inspect data before testing**\n",
    "   - Check mean-variance relationship\n",
    "   - Assess sparsity\n",
    "   - Verify group sizes\n",
    "\n",
    "3. **Use multiple methods for validation**\n",
    "   - Cross-method validation increases confidence\n",
    "   - Focus on consensus findings\n",
    "   - Report method used in publications\n",
    "\n",
    "4. **Report adjusted p-values (FDR)**\n",
    "   - Always report FDR-adjusted p-values\n",
    "   - Specify correction method (Benjamini-Hochberg default)\n",
    "   - Include fold change thresholds\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Scenario | Recommended Method | Function |\n",
    "|----------|-------------------|----------|\n",
    "| Count data, small sample | VOOM | `diff_expr_voom()` |\n",
    "| Count data, mean-variance trend | limma-trend | `diff_expr_limma_trend()` |\n",
    "| Count data, over-dispersed | DESeq2 | `diff_expr_deseq2()` |\n",
    "| Non-normal distribution | Wilcoxon | `diff_expr_wilcoxon()` |\n",
    "| Unequal variances | Brunner-Munzel | `diff_expr_brunner_munzel()` |\n",
    "| Paired samples | Wilcoxon paired | `diff_expr_wilcoxon(paired=True)` |\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "- Using parametric tests on highly non-normal data\n",
    "- Ignoring the mean-variance relationship in count data\n",
    "- Not filtering low-count features before count-based analysis\n",
    "- Forgetting to use paired tests for matched samples\n",
    "- Over-interpreting single-method results\n",
    "\n",
    "### Next Steps\n",
    "- **Tutorial 10**: [Advanced Topics]\n",
    "- **Documentation**: See `docs/design/API_REFERENCE.md` for full API details\n",
    "- **Examples**: Check `examples/` for more usage patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary of all results\n",
    "print(\"=\" * 70)\n",
    "print(\"DIFFERENTIAL EXPRESSION ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_data = []\n",
    "for name, result in results.items():\n",
    "    n_tested = np.sum(~np.isnan(result.p_values))\n",
    "    n_sig = np.sum(result.p_values_adj < 0.05)\n",
    "    n_up = np.sum((result.p_values_adj < 0.05) & (result.log2_fc > 1))\n",
    "    n_down = np.sum((result.p_values_adj < 0.05) & (result.log2_fc < -1))\n",
    "    summary_data.append([name, n_tested, n_sig, n_up, n_down])\n",
    "\n",
    "summary_df = pl.DataFrame(\n",
    "    summary_data, schema=[\"Method\", \"Tested\", \"Significant\", \"Up in A\", \"Down in A\"]\n",
    ")\n",
    "print(summary_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Analysis complete! Results saved to tutorial_output/\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
