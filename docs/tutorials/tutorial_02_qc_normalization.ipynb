{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02: Quality Control and Normalization\n",
    "\n",
    "This tutorial covers quality control (QC) and normalization techniques for single-cell proteomics data.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Calculate and visualize QC metrics\n",
    "- Detect and filter low-quality samples and features\n",
    "- Apply various normalization methods\n",
    "- Compare normalization approaches\n",
    "- Understand the impact of normalization on data distribution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import required libraries and load an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply SciencePlots style\n",
    "plt.style.use([\"science\", \"no-latex\"])\n",
    "\n",
    "# Import ScpTensor\n",
    "import scptensor\n",
    "from scptensor.datasets import load_simulated_scrnaseq_like\n",
    "from scptensor import (\n",
    "    # QC functions\n",
    "    calculate_qc_metrics,\n",
    "    detect_outliers,\n",
    "    filter_samples_by_missing_rate,\n",
    "    filter_samples_by_total_count,\n",
    "    filter_features_by_missing_rate,\n",
    "    filter_features_by_variance,\n",
    "    # Normalization functions\n",
    "    log_normalize,\n",
    "    sample_median_normalization,\n",
    "    global_median_normalization,\n",
    "    tmm_normalization,\n",
    "    upper_quartile_normalization,\n",
    "    zscore,\n",
    ")\n",
    "\n",
    "print(f\"ScpTensor version: {scptensor.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load a Larger Dataset\n",
    "\n",
    "For this tutorial, we'll use a larger simulated dataset that includes QC metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulated dataset\n",
    "container = load_simulated_scrnaseq_like()\n",
    "\n",
    "print(f\"Dataset loaded: {container}\")\n",
    "print(f\"\\nSamples: {container.n_samples}\")\n",
    "print(f\"Features: {container.assays['proteins'].n_features}\")\n",
    "print(f\"\\nAvailable columns in obs:\")\n",
    "print(container.obs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output:\n",
    "```\n",
    "Dataset loaded: ScpContainer with 500 samples and 1 assay\n",
    "\n",
    "Samples: 500\n",
    "Features: 200\n",
    "\n",
    "Available columns in obs:\n",
    "['sample_id', 'batch', 'cell_type', 'batch_id', 'cell_type_id', \n",
    " 'n_detected', 'missing_rate', 'total_intensity', 'mean_intensity', \n",
    " 'median_intensity', 'mad_intensity']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quality Control Metrics\n",
    "\n",
    "### 3.1 Calculate QC Metrics\n",
    "\n",
    "Let's calculate comprehensive QC metrics for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate QC metrics\n",
    "container = calculate_qc_metrics(container, assay_name=\"proteins\")\n",
    "\n",
    "# View the added QC metrics\n",
    "print(\"QC metrics added to obs:\")\n",
    "print(\"=\" * 50)\n",
    "print(container.obs.select([\n",
    "    \"sample_id\", \"n_detected\", \"missing_rate\", \n",
    "    \"total_intensity\", \"mean_intensity\"\n",
    "]).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize QC Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QC visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# Missing rate distribution\n",
    "axes[0, 0].hist(container.obs[\"missing_rate\"].to_numpy(), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Missing Rate')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Missing Rate per Sample')\n",
    "\n",
    "# Number of detected features\n",
    "axes[0, 1].hist(container.obs[\"n_detected\"].to_numpy(), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Number of Detected Proteins')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Detected Proteins')\n",
    "\n",
    "# Total intensity\n",
    "axes[1, 0].hist(container.obs[\"total_intensity\"].to_numpy(), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Total Intensity')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Total Intensity')\n",
    "\n",
    "# Mean intensity\n",
    "axes[1, 1].hist(container.obs[\"mean_intensity\"].to_numpy(), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Mean Intensity')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Mean Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tutorial_output/qc_distributions.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"QC plots saved to: tutorial_output/qc_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 QC by Batch and Cell Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group statistics by batch and cell type\n",
    "print(\"QC Statistics by Batch:\")\n",
    "print(\"=\" * 50)\n",
    "batch_stats = container.obs.group_by(\"batch\").agg(\n",
    "    pl.col(\"missing_rate\").mean().alias(\"mean_missing\"),\n",
    "    pl.col(\"n_detected\").mean().alias(\"mean_detected\"),\n",
    "    pl.col(\"total_intensity\").mean().alias(\"mean_total_intensity\"),\n",
    "    pl.len().alias(\"n_samples\")\n",
    ").sort(\"batch\")\n",
    "print(batch_stats)\n",
    "\n",
    "print(\"\\nQC Statistics by Cell Type:\")\n",
    "print(\"=\" * 50)\n",
    "celltype_stats = container.obs.group_by(\"cell_type\").agg(\n",
    "    pl.col(\"missing_rate\").mean().alias(\"mean_missing\"),\n",
    "    pl.col(\"n_detected\").mean().alias(\"mean_detected\"),\n",
    "    pl.col(\"total_intensity\").mean().alias(\"mean_total_intensity\"),\n",
    "    pl.len().alias(\"n_samples\")\n",
    ").sort(\"cell_type\")\n",
    "print(celltype_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outlier Detection\n",
    "\n",
    "### 4.1 Detect Outlier Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using median absolute deviation (MAD)\n",
    "outliers = detect_outliers(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    metric=\"mad\",  # Options: 'mad', 'zscore', 'iqr'\n",
    "    threshold=3.0,  # MAD threshold\n",
    "    key_added=\"is_outlier\"\n",
    ")\n",
    "\n",
    "print(f\"Outlier detection completed.\")\n",
    "print(f\"Number of outliers detected: {outliers.obs['is_outlier'].sum()}\")\n",
    "print(f\"Outlier percentage: {outliers.obs['is_outlier'].mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualize Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total intensity with outliers highlighted\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "total_intensity = outliers.obs[\"total_intensity\"].to_numpy()\n",
    "is_outlier = outliers.obs[\"is_outlier\"].to_numpy()\n",
    "\n",
    "# Plot non-outliers\n",
    "ax.scatter(np.where(~is_outlier)[0], total_intensity[~is_outlier], \n",
    "           c='blue', alpha=0.6, s=30, label='Normal')\n",
    "\n",
    "# Plot outliers\n",
    "ax.scatter(np.where(is_outlier)[0], total_intensity[is_outlier], \n",
    "           c='red', alpha=0.8, s=50, label='Outlier', marker='x')\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Total Intensity')\n",
    "ax.set_title('Outlier Detection Based on Total Intensity')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tutorial_output/outlier_detection.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Outlier plot saved to: tutorial_output/outlier_detection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filtering\n",
    "\n",
    "### 5.1 Filter Samples by Missing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter samples with >50% missing values\n",
    "print(\"Before filtering:\")\n",
    "print(f\"  Samples: {container.n_samples}\")\n",
    "print(f\"  Features: {container.assays['proteins'].n_features}\")\n",
    "\n",
    "container_filtered = filter_samples_by_missing_rate(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    threshold=0.5,  # Keep samples with <=50% missing\n",
    ")\n",
    "\n",
    "print(\"\\nAfter filtering by missing rate:\")\n",
    "print(f\"  Samples: {container_filtered.n_samples}\")\n",
    "print(f\"  Features: {container_filtered.assays['proteins'].n_features}\")\n",
    "print(f\"  Samples removed: {container.n_samples - container_filtered.n_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Filter Samples by Total Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter samples with low total intensity (bottom 5%)\n",
    "min_intensity = np.percentile(container_filtered.obs[\"total_intensity\"].to_numpy(), 5)\n",
    "\n",
    "container_filtered = filter_samples_by_total_count(\n",
    "    container_filtered,\n",
    "    assay_name=\"proteins\",\n",
    "    min_total_count=min_intensity,\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter filtering by total count:\")\n",
    "print(f\"  Samples: {container_filtered.n_samples}\")\n",
    "print(f\"  Features: {container_filtered.assays['proteins'].n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Filter Features by Missing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter proteins detected in <20% of samples\n",
    "print(\"Before feature filtering:\")\n",
    "print(f\"  Features: {container_filtered.assays['proteins'].n_features}\")\n",
    "\n",
    "container_filtered = filter_features_by_missing_rate(\n",
    "    container_filtered,\n",
    "    assay_name=\"proteins\",\n",
    "    threshold=0.8,  # Keep features with <=80% missing (detected in >=20%)\n",
    ")\n",
    "\n",
    "print(\"\\nAfter feature filtering:\")\n",
    "print(f\"  Samples: {container_filtered.n_samples}\")\n",
    "print(f\"  Features: {container_filtered.assays['proteins'].n_features}\")\n",
    "print(f\"  Features removed: {container.assays['proteins'].n_features - container_filtered.assays['proteins'].n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Filter Features by Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter low-variance features (bottom 10%)\n",
    "container_filtered = filter_features_by_variance(\n",
    "    container_filtered,\n",
    "    assay_name=\"proteins\",\n",
    "    percentile=10,  # Remove bottom 10% variance\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter filtering by variance:\")\n",
    "print(f\"  Samples: {container_filtered.n_samples}\")\n",
    "print(f\"  Features: {container_filtered.assays['proteins'].n_features}\")\n",
    "print(f\"\\nFinal data shape: {container_filtered.n_samples} x {container_filtered.assays['proteins'].n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Normalization\n",
    "\n",
    "Normalization removes technical variation (e.g., sample-specific effects) to make samples comparable.\n",
    "\n",
    "### 6.1 Data Distribution Before Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw data\n",
    "X_raw = container_filtered.assays[\"proteins\"].layers[\"raw\"].X\n",
    "M_raw = container_filtered.assays[\"proteins\"].layers[\"raw\"].M\n",
    "\n",
    "# Create masked array for visualization\n",
    "X_raw_masked = X_raw.copy().astype(float)\n",
    "X_raw_masked[M_raw != 0] = np.nan\n",
    "\n",
    "# Plot distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Density plot\n",
    "axes[0].hist(X_raw_masked.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Intensity')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Raw Data Distribution')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Box plot by sample\n",
    "sample_medians = np.nanmedian(X_raw_masked, axis=1)\n",
    "axes[1].boxplot(sample_medians, vert=False)\n",
    "axes[1].set_xlabel('Median Intensity')\n",
    "axes[1].set_yticks([])\n",
    "axes[1].set_title('Distribution of Sample Medians (Raw)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tutorial_output/before_normalization.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Raw data statistics:\")\n",
    "print(f\"  Mean: {np.nanmean(X_raw_masked):.4f}\")\n",
    "print(f\"  Median: {np.nanmedian(X_raw_masked):.4f}\")\n",
    "print(f\"  Std: {np.nanstd(X_raw_masked):.4f}\")\n",
    "print(f\"  CV: {np.nanstd(X_raw_masked) / np.nanmean(X_raw_masked) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Log Normalization\n",
    "\n",
    "Log transformation stabilizes variance and makes the data more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log2 normalization with offset\n",
    "container_normalized = log_normalize(\n",
    "    container_filtered,\n",
    "    assay_name=\"proteins\",\n",
    "    base_layer=\"raw\",\n",
    "    new_layer_name=\"log\",\n",
    "    base=2.0,  # Log base\n",
    "    offset=1.0,  # Pseudocount to avoid log(0)\n",
    ")\n",
    "\n",
    "# Check that log layer was created\n",
    "print(\"Layers after log normalization:\")\n",
    "print(list(container_normalized.assays['proteins'].layers.keys()))\n",
    "\n",
    "# Get log-transformed data\n",
    "X_log = container_normalized.assays[\"proteins\"].layers[\"log\"].X\n",
    "\n",
    "# Plot distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(X_log.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Log2 Intensity')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Log Normalized Data Distribution')\n",
    "\n",
    "sample_medians_log = np.nanmedian(X_log, axis=1)\n",
    "axes[1].boxplot(sample_medians_log, vert=False)\n",
    "axes[1].set_xlabel('Median Log2 Intensity')\n",
    "axes[1].set_yticks([])\n",
    "axes[1].set_title('Distribution of Sample Medians (Log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tutorial_output/log_normalization.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLog normalized statistics:\")\n",
    "print(f\"  Mean: {np.nanmean(X_log):.4f}\")\n",
    "print(f\"  Median: {np.nanmedian(X_log):.4f}\")\n",
    "print(f\"  Std: {np.nanstd(X_log):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Median Normalization\n",
    "\n",
    "Median normalization scales each sample to have the same median intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sample median normalization to log data\n",
    "container_normalized = sample_median_normalization(\n",
    "    container_normalized,\n",
    "    assay_name=\"proteins\",\n",
    "    base_layer=\"log\",\n",
    "    new_layer_name=\"log_median\",\n",
    ")\n",
    "\n",
    "X_log_median = container_normalized.assays[\"proteins\"].layers[\"log_median\"].X\n",
    "\n",
    "# Plot sample medians before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].boxplot([sample_medians_log, np.nanmedian(X_log_median, axis=1)], vert=False)\n",
    "axes[0].set_yticklabels(['Log', 'Log + Median'])\n",
    "axes[0].set_xlabel('Median Intensity')\n",
    "axes[0].set_title('Sample Medians Comparison')\n",
    "\n",
    "# CV comparison\n",
    "cv_before = np.std(sample_medians_log) / np.mean(sample_medians_log) * 100\n",
    "cv_after = np.nanstd(X_log_median, axis=1).std() / np.nanmean(X_log_median) * 100\n",
    "\n",
    "axes[1].bar(['Before', 'After'], [cv_before, cv_after], color=['coral', 'skyblue'])\n",
    "axes[1].set_ylabel('Coefficient of Variation (%)')\n",
    "axes[1].set_title('Cross-Sample Variation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tutorial_output/median_normalization.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"CV before median normalization: {cv_before:.2f}%\")\n",
    "print(f\"CV after median normalization: {cv_after:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 TMM Normalization (Trimmed Mean of M-values)\n",
    "\n",
    "TMM is a robust normalization method commonly used in proteomics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TMM normalization\n",
    "container_normalized = tmm_normalization(\n",
    "    container_normalized,\n",
    "    assay_name=\"proteins\",\n",
    "    base_layer=\"raw\",\n",
    "    new_layer_name=\"tmm\",\n",
    "    trim_ratio=0.3,  # Trim 30% from each end\n",
    ")\n",
    "\n",
    "X_tmm = container_normalized.assays[\"proteins\"].layers[\"tmm\"].X\n",
    "\n",
    "print(\"TMM normalization completed.\")\n",
    "print(f\"TMM data shape: {X_tmm.shape}\")\n",
    "print(f\"Mean intensity: {np.nanmean(X_tmm):.4f}\")\n",
    "print(f\"Median intensity: {np.nanmedian(X_tmm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Upper Quartile Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply upper quartile normalization\n",
    "container_normalized = upper_quartile_normalization(\n",
    "    container_normalized,\n",
    "    assay_name=\"proteins\",\n",
    "    base_layer=\"raw\",\n",
    "    new_layer_name=\"uq\",\n",
    "    percentile=0.75,  # Upper quartile (75th percentile)\n",
    ")\n",
    "\n",
    "X_uq = container_normalized.assays[\"proteins\"].layers[\"uq\"].X\n",
    "\n",
    "print(\"Upper quartile normalization completed.\")\n",
    "print(f\"UQ data shape: {X_uq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Z-Score Standardization\n",
    "\n",
    "Z-score standardization scales each feature to have mean=0 and std=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply z-score standardization\n",
    "container_normalized = zscore(\n",
    "    container_normalized,\n",
    "    assay_name=\"proteins\",\n",
    "    base_layer=\"log\",\n",
    "    new_layer_name=\"zscore\",\n",
    ")\n",
    "\n",
    "X_zscore = container_normalized.assays[\"proteins\"].layers[\"zscore\"].X\n",
    "\n",
    "print(\"Z-score standardization completed.\")\n",
    "print(f\"Mean (should be ~0): {np.nanmean(X_zscore):.6f}\")\n",
    "print(f\"Std (should be ~1): {np.nanstd(X_zscore):.6f}\")\n",
    "\n",
    "# Plot z-scored data\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.hist(X_zscore.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Z-Score')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Z-Score Standardized Data Distribution')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tutorial_output/zscore_distribution.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Normalization Methods\n",
    "\n",
    "Let's compare the effects of different normalization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sample medians for each method\n",
    "layers = ['raw', 'log', 'log_median', 'tmm', 'uq']\n",
    "sample_medians_by_method = {}\n",
    "\n",
    "for layer in layers:\n",
    "    if layer in container_normalized.assays['proteins'].layers:\n",
    "        X = container_normalized.assays['proteins'].layers[layer].X\n",
    "        sample_medians_by_method[layer] = np.nanmedian(X, axis=1)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot comparison\n",
    "data_to_plot = [sample_medians_by_method[layer] for layer in layers if layer in sample_medians_by_method]\n",
    "labels_to_plot = [layer for layer in layers if layer in sample_medians_by_method]\n",
    "bp = axes[0].boxplot(data_to_plot, labels=labels_to_plot)\n",
    "axes[0].set_ylabel('Median Intensity')\n",
    "axes[0].set_title('Comparison of Normalization Methods')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# CV comparison\n",
    "cv_values = [np.std(d) / np.mean(d) * 100 for d in data_to_plot]\n",
    "axes[1].bar(labels_to_plot, cv_values, color='steelblue')\n",
    "axes[1].set_ylabel('Coefficient of Variation (%)')\n",
    "axes[1].set_title('Cross-Sample Variation by Method')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tutorial_output/normalization_comparison.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCoefficient of Variation by method:\")\n",
    "for layer, cv in zip(labels_to_plot, cv_values):\n",
    "    print(f\"  {layer:15s}: {cv:6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing Normalization Effects\n",
    "\n",
    "Let's visualize how the data distribution changes with normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first 100 features for visualization\n",
    "n_features_plot = min(100, container_normalized.assays['proteins'].n_features)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "methods = ['raw', 'log', 'log_median', 'tmm', 'uq', 'zscore']\n",
    "titles = ['Raw', 'Log2', 'Log + Median', 'TMM', 'Upper Quartile', 'Z-Score']\n",
    "\n",
    "for i, (method, title) in enumerate(zip(methods, titles)):\n",
    "    if method in container_normalized.assays['proteins'].layers:\n",
    "        X = container_normalized.assays['proteins'].layers[method].X\n",
    "        \n",
    "        # Plot first 100 features\n",
    "        for j in range(n_features_plot):\n",
    "            axes[i].plot(X[:, j], alpha=0.1, color='blue')\n",
    "        \n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel('Sample Index')\n",
    "        if i == 0:\n",
    "            axes[i].set_ylabel('Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tutorial_output/normalization_profiles.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Normalization profiles saved to: tutorial_output/normalization_profiles.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "### Quality Control:\n",
    "1. **Calculate QC Metrics**: Using `calculate_qc_metrics()`\n",
    "2. **Detect Outliers**: Using `detect_outliers()` with MAD, Z-score, or IQR methods\n",
    "3. **Filter Samples**: By missing rate (`filter_samples_by_missing_rate()`) and total count (`filter_samples_by_total_count()`)\n",
    "4. **Filter Features**: By missing rate (`filter_features_by_missing_rate()`) and variance (`filter_features_by_variance()`)\n",
    "\n",
    "### Normalization:\n",
    "1. **Log Normalization**: Stabilizes variance (`log_normalize()`)\n",
    "2. **Median Normalization**: Scales samples to common median (`sample_median_normalization()`)\n",
    "3. **TMM Normalization**: Robust normalization for proteomics (`tmm_normalization()`)\n",
    "4. **Upper Quartile Normalization**: Uses 75th percentile (`upper_quartile_normalization()`)\n",
    "5. **Z-Score Standardization**: Mean=0, Std=1 (`zscore()`)\n",
    "\n",
    "### Best Practices:\n",
    "- Always QC before normalization\n",
    "- Log transform before most normalization methods\n",
    "- Choose normalization based on your data characteristics\n",
    "- Use z-score for methods that assume standardized data (e.g., PCA, clustering)\n",
    "\n",
    "### Next Steps:\n",
    "- **Tutorial 03**: Imputation and Batch Correction\n",
    "- **Tutorial 04**: Clustering and Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
