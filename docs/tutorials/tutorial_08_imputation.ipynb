{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 08: Comprehensive Guide to Missing Value Imputation\n",
    "\n",
    "This tutorial provides a comprehensive guide to missing value imputation methods for single-cell proteomics data.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand different types of missing values (MCAR, MAR, MNAR)\n",
    "- Learn about all 10 available imputation methods in ScpTensor\n",
    "- Know when to use each imputation method\n",
    "- Apply each method with code examples\n",
    "- Visualize and compare imputation results\n",
    "- Evaluate imputation quality\n",
    "- Follow best practices for imputation workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Missing Values](#1-understanding-missing-values)\n",
    "2. [Imputation Methods Overview](#2-imputation-methods-overview)\n",
    "3. [MCAR Methods - Missing Completely At Random](#3-mcar-methods)\n",
    "4. [MNAR Methods - Missing Not At Random](#4-mnar-methods)\n",
    "5. [Advanced Methods](#5-advanced-methods)\n",
    "6. [Performance Comparison](#6-performance-comparison)\n",
    "7. [Best Practices](#7-best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Missing Values\n",
    "\n",
    "### 1.1 Types of Missing Values\n",
    "\n",
    "Single-cell proteomics data contains different types of missing values:\n",
    "\n",
    "| Type | Full Name | Description | Common Cause | Recommended Methods |\n",
    "|------|-----------|-------------|--------------|---------------------|\n",
    "| **MCAR** | Missing Completely At Random | Missingness unrelated to data | Technical dropout, random failures | KNN, PPCA, SVD, LLS, BPCA, MissForest |\n",
    "| **MAR** | Missing At Random | Missingness related to observed data | Systematic bias, batch effects | KNN, MissForest, BPCA |\n",
    "| **MNAR** | Missing Not At Random | Missingness related to unobserved value | Below detection limit (LOD) | QRILC, MinProb, MinDet |\n",
    "\n",
    "### 1.2 Mask Codes in ScpTensor\n",
    "\n",
    "ScpTensor uses mask codes to track the origin of each value:\n",
    "\n",
    "| Code | Name | Description |\n",
    "|------|------|-------------|\n",
    "| 0 | VALID | Valid detected value |\n",
    "| 1 | MBR | Missing Between Runs (technical variation) |\n",
    "| 2 | LOD | Limit of Detection (below threshold) |\n",
    "| 3 | FILTERED | Removed by quality control |\n",
    "| 5 | IMPUTED | Filled by imputation method |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Setup\n",
    "\n",
    "First, let's import the required libraries and load a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Apply SciencePlots style\n",
    "plt.style.use([\"science\", \"no-latex\"])\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# Import ScpTensor\n",
    "import scptensor\n",
    "from scptensor import (\n",
    "    # Utilities\n",
    "    count_mask_codes,\n",
    "    impute_bpca,\n",
    "    # All imputation methods\n",
    "    impute_knn,\n",
    "    impute_lls,\n",
    "    impute_mf,\n",
    "    impute_mindet,\n",
    "    impute_minprob,\n",
    "    impute_ppca,\n",
    "    impute_qrilc,\n",
    "    impute_svd,\n",
    "    norm_log,\n",
    ")\n",
    "from scptensor.datasets import load_simulated_scrnaseq_like\n",
    "\n",
    "print(f\"ScpTensor version: {scptensor.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "container = load_simulated_scrnaseq_like()\n",
    "\n",
    "print(f\"Dataset loaded: {container}\")\n",
    "print(f\"Samples: {container.n_samples}\")\n",
    "print(f\"Features: {container.assays['proteins'].n_features}\")\n",
    "print(f\"Available layers: {list(container.assays['proteins'].layers.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Analyze Missing Value Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing value patterns\n",
    "matrix = container.assays[\"proteins\"].layers[\"raw\"]\n",
    "mask_counts = count_mask_codes(matrix.M)\n",
    "\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total values: {matrix.M.size}\")\n",
    "for code, name in [(0, \"VALID\"), (1, \"MBR\"), (2, \"LOD\"), (3, \"FILTERED\"), (5, \"IMPUTED\")]:\n",
    "    count = mask_counts.get(code, 0)\n",
    "    pct = count / matrix.M.size * 100\n",
    "    print(f\"{name} ({code}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Overall missing rate\n",
    "missing_rate = (matrix.M != 0).sum() / matrix.M.size\n",
    "print(f\"\\nOverall missing rate: {missing_rate * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Visualize Missing Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing value patterns\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = GridSpec(3, 3, figure=fig)\n",
    "\n",
    "# Missing rate per sample\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sample_missing = (matrix.M != 0).sum(axis=1) / matrix.M.shape[1]\n",
    "ax1.bar(range(len(sample_missing)), sample_missing, color=\"steelblue\", alpha=0.7)\n",
    "ax1.set_xlabel(\"Sample Index\")\n",
    "ax1.set_ylabel(\"Missing Rate\")\n",
    "ax1.set_title(\"Missing Rate per Sample\")\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Missing rate per feature\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "feature_missing = (matrix.M != 0).sum(axis=0) / matrix.M.shape[0]\n",
    "ax2.hist(feature_missing, bins=30, color=\"coral\", edgecolor=\"black\", alpha=0.7)\n",
    "ax2.set_xlabel(\"Missing Rate\")\n",
    "ax2.set_ylabel(\"Number of Features\")\n",
    "ax2.set_title(\"Distribution of Missing Rate per Feature\")\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Overall missing distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "mask_labels = [\"VALID\", \"MBR\", \"LOD\", \"FILTERED\"]\n",
    "mask_values = [mask_counts.get(i, 0) for i in [0, 1, 2, 3]]\n",
    "colors_pie = [\"lightgreen\", \"orange\", \"red\", \"gray\"]\n",
    "ax3.pie(mask_values, labels=mask_labels, colors=colors_pie, autopct=\"%1.1f%%\", startangle=90)\n",
    "ax3.set_title(\"Mask Code Distribution\")\n",
    "\n",
    "# Spy plot (missing pattern) - subset\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "n_show = min(100, matrix.M.shape[0], matrix.M.shape[1])\n",
    "missing_mask = (matrix.M[:n_show, :n_show] != 0).astype(float)\n",
    "ax4.imshow(missing_mask, aspect=\"auto\", cmap=\"Reds\", interpolation=\"none\")\n",
    "ax4.set_xlabel(f\"Feature Index (1-{n_show})\")\n",
    "ax4.set_ylabel(f\"Sample Index (1-{n_show})\")\n",
    "ax4.set_title(\"Missing Value Pattern (Spy Plot)\")\n",
    "\n",
    "# Missing by mask code type\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "x_pos = np.arange(matrix.M.shape[1])\n",
    "width = 0.6\n",
    "lod_rate = (matrix.M[:, :n_show] == 2).sum(axis=0) / matrix.M.shape[0]\n",
    "mbr_rate = (matrix.M[:, :n_show] == 1).sum(axis=0) / matrix.M.shape[0]\n",
    "ax5.bar(x_pos, lod_rate[:n_show], width, label=\"LOD (MNAR)\", color=\"coral\", alpha=0.7)\n",
    "ax5.bar(\n",
    "    x_pos,\n",
    "    mbr_rate[:n_show],\n",
    "    width,\n",
    "    bottom=lod_rate[:n_show],\n",
    "    label=\"MBR (MCAR)\",\n",
    "    color=\"steelblue\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax5.set_xlabel(\"Feature Index\")\n",
    "ax5.set_ylabel(\"Missing Rate\")\n",
    "ax5.set_title(\"Missing Type by Feature\")\n",
    "ax5.legend()\n",
    "ax5.set_xlim(-0.5, n_show - 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/imputation_missing_patterns.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Missing pattern visualizations saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Imputation Methods Overview\n",
    "\n",
    "ScpTensor provides **10 imputation methods** for different scenarios:\n",
    "\n",
    "### 2.1 Method Selection Guide\n",
    "\n",
    "```python\n",
    "# Quick decision tree:\n",
    "if missingness_type == \"MCAR\":  # Technical dropout\n",
    "    if n_samples < 1000:\n",
    "        use = \"KNN\"  # Fast, accurate for small data\n",
    "    else:\n",
    "        use = \"PPCA\"  # Scalable to large data\n",
    "elif missingness_type == \"MNAR\":  # Below detection limit\n",
    "    if want_best_accuracy:\n",
    "        use = \"QRILC\"  # Best for left-censored data\n",
    "    else:\n",
    "        use = \"MinProb\"  # Faster, simpler\n",
    "elif high_correlation:\n",
    "    use = \"LLS\"  # Exploits feature correlations\n",
    "else:\n",
    "    use = \"BPCA\"  # Robust, automatic regularization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Method Comparison Table\n",
    "\n",
    "| Method | Full Name | Missing Type | Speed | Accuracy | Best For |\n",
    "|--------|-----------|--------------|-------|----------|----------|\n",
    "| **KNN** | K-Nearest Neighbors | MCAR | Fast | Good | Small datasets, general use |\n",
    "| **PPCA** | Probabilistic PCA | MCAR | Medium | Good | Large datasets, linear structure |\n",
    "| **BPCA** | Bayesian PCA | MCAR | Slow | Excellent | Automatic model selection |\n",
    "| **SVD** | Singular Value Decomposition | MCAR | Fast | Good | Dimensionality reduction |\n",
    "| **MissForest** | Random Forest | MCAR/MAR | Very Slow | Excellent | Complex patterns |\n",
    "| **LLS** | Local Least Squares | MCAR | Medium | Excellent | Correlated features |\n",
    "| **QRILC** | Quantile Regression Imputation of Left-Censored Data | MNAR | Medium | Excellent | Below detection limit |\n",
    "| **MinProb** | Probabilistic Minimum | MNAR | Fast | Good | Fast MNAR imputation |\n",
    "| **MinDet** | Deterministic Minimum | MNAR | Very Fast | Fair | Quick baseline |\n",
    "| **NMF** | Non-negative Matrix Factorization | MCAR | Medium | Good | Non-negative data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. MCAR Methods - Missing Completely At Random\n",
    "\n",
    "These methods assume missingness is unrelated to the data values.\n",
    "\n",
    "### 3.1 Preprocessing: Log Normalization\n",
    "\n",
    "Before imputation, we typically apply log normalization to stabilize variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log normalization\n",
    "container = norm_log(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    base_layer=\"raw\",\n",
    "    new_layer_name=\"log\",\n",
    "    base=2.0,\n",
    "    offset=1.0,\n",
    ")\n",
    "\n",
    "print(\"Log normalization completed.\")\n",
    "print(f\"Available layers: {list(container.assays['proteins'].layers.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 K-Nearest Neighbors (KNN) Imputation\n",
    "\n",
    "**Algorithm:** For each sample with missing values, find k nearest neighbors and impute using the average of their values.\n",
    "\n",
    "**Best for:** Small to medium datasets, general-purpose imputation.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `k`: Number of neighbors (default: 10)\n",
    "- Larger k = more smoothing, less local variation\n",
    "- Smaller k = more sensitive to noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KNN imputation\n",
    "print(\"Running KNN imputation...\")\n",
    "\n",
    "container = impute_knn(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    base_layer=\"log\",\n",
    "    new_layer_name=\"knn_imputed\",\n",
    "    k=10,  # Number of neighbors\n",
    ")\n",
    "\n",
    "print(\"KNN imputation completed.\")\n",
    "\n",
    "# Check results\n",
    "knn_matrix = container.assays[\"proteins\"].layers[\"knn_imputed\"]\n",
    "knn_missing_rate = (knn_matrix.M == 5).sum() / knn_matrix.M.size\n",
    "print(f\"Imputed values marked: {knn_missing_rate * 100:.2f}%\")\n",
    "print(f\"Imputed count: {(knn_matrix.M == 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Probabilistic PCA (PPCA) Imputation\n",
    "\n",
    "**Algorithm:** Models data as x = Wz + mu + epsilon using EM algorithm. Iteratively estimates latent variables and parameters.\n",
    "\n",
    "**Best for:** Large datasets, data with linear structure.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `n_components`: Number of principal components\n",
    "- `max_iter`: Maximum EM iterations\n",
    "- `tol`: Convergence tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PPCA imputation\n",
    "print(\"Running PPCA imputation...\")\n",
    "\n",
    "container = impute_ppca(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    base_layer=\"log\",\n",
    "    new_layer_name=\"ppca_imputed\",\n",
    "    n_components=10,  # Number of principal components\n",
    "    max_iter=100,\n",
    ")\n",
    "\n",
    "print(\"PPCA imputation completed.\")\n",
    "\n",
    "# Check results\n",
    "ppca_matrix = container.assays[\"proteins\"].layers[\"ppca_imputed\"]\n",
    "print(f\"Imputed values: {(ppca_matrix.M == 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bayesian PCA (BPCA) Imputation\n",
    "\n",
    "**Algorithm:** Extends PPCA with Bayesian inference and Automatic Relevance Determination (ARD) priors. Automatically determines effective number of components.\n",
    "\n",
    "**Best for:** Automatic model selection, avoiding overfitting.\n",
    "\n",
    "**Key Features:**\n",
    "- Uses ARD priors to prune unnecessary components\n",
    "- More robust than standard PPCA\n",
    "- Automatic determination of effective dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply BPCA imputation\n",
    "print(\"Running BPCA imputation...\")\n",
    "\n",
    "container = impute_bpca(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    base_layer=\"log\",\n",
    "    new_layer_name=\"bpca_imputed\",\n",
    "    n_components=10,  # Maximum components\n",
    "    max_iter=100,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"BPCA imputation completed.\")\n",
    "\n",
    "# Check results\n",
    "bpca_matrix = container.assays[\"proteins\"].layers[\"bpca_imputed\"]\n",
    "print(f\"Imputed values: {(bpca_matrix.M == 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 SVD Imputation\n",
    "\n",
    "**Algorithm:** Iterative SVD - alternates between SVD decomposition and missing value imputation.\n",
    "\n",
    "**Best for:** Large datasets, fast dimensionality reduction.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `rank`: Rank for SVD approximation\n",
    "- `max_iter`: Maximum iterations\n",
    "- `tol`: Convergence tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD imputation\n",
    "print(\"Running SVD imputation...\")\n",
    "\n",
    "container = impute_svd(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    base_layer=\"log\",\n",
    "    new_layer_name=\"svd_imputed\",\n",
    "    rank=10,  # SVD rank\n",
    ")\n",
    "\n",
    "print(\"SVD imputation completed.\")\n",
    "\n",
    "# Check results\n",
    "svd_matrix = container.assays[\"proteins\"].layers[\"svd_imputed\"]\n",
    "print(f\"Imputed values: {(svd_matrix.M == 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. MNAR Methods - Missing Not At Random\n",
    "\n",
    "These methods are designed for left-censored data where missingness is due to values being below the detection limit (LOD).\n",
    "\n",
    "### 4.1 QRILC (Quantile Regression Imputation of Left-Censored Data)\n",
    "\n",
    "**Algorithm:** For each feature, estimates the detection limit and models detected values with a normal distribution. Samples from the left-censored tail for missing values.\n",
    "\n",
    "**Best for:** MNAR data, below detection limit missingness.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `q`: Quantile for detection limit (0-1, default: 0.01)\n",
    "- Lower q = more aggressive censoring threshold\n",
    "- `random_state`: Random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply QRILC imputation\n",
    "print(\"Running QRILC imputation...\")\n",
    "\n",
    "container = impute_qrilc(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    source_layer=\"log\",\n",
    "    new_layer_name=\"qrilc_imputed\",\n",
    "    q=0.01,  # Left-censoring quantile\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"QRILC imputation completed.\")\n",
    "\n",
    "# Check results\n",
    "qrilc_matrix = container.assays[\"proteins\"].layers[\"qrilc_imputed\"]\n",
    "print(f\"Imputed values: {(qrilc_matrix.M == 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 MinProb (Probabilistic Minimum) Imputation\n",
    "\n",
    "**Algorithm:** For each feature, samples from a distribution centered at the minimum detected value.\n",
    "\n",
    "**Best for:** Fast MNAR imputation, preserves uncertainty.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `sigma`: Distribution width multiplier (default: 2.0)\n",
    "- Larger sigma = wider distribution\n",
    "- `random_state`: Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinProb imputation\n",
    "print(\"Running MinProb imputation...\")\n",
    "\n",
    "container = impute_minprob(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    source_layer=\"log\",\n",
    "    new_layer_name=\"minprob_imputed\",\n",
    "    sigma=2.0,  # Distribution width\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"MinProb imputation completed.\")\n",
    "\n",
    "# Check results\n",
    "minprob_matrix = container.assays[\"proteins\"].layers[\"minprob_imputed\"]\n",
    "print(f\"Imputed values: {(minprob_matrix.M == 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 MinDet (Deterministic Minimum) Imputation\n",
    "\n",
    "**Algorithm:** Uses a fixed deterministic value for all missing values in each feature.\n",
    "\n",
    "**Best for:** Quick baseline, reproducible results.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `sigma`: Controls how far below minimum to impute (default: 1.0)\n",
    "- No random_state needed (deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinDet imputation\n",
    "print(\"Running MinDet imputation...\")\n",
    "\n",
    "container = impute_mindet(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    source_layer=\"log\",\n",
    "    new_layer_name=\"mindet_imputed\",\n",
    "    sigma=1.0,\n",
    ")\n",
    "\n",
    "print(\"MinDet imputation completed.\")\n",
    "\n",
    "# Check results\n",
    "mindet_matrix = container.assays[\"proteins\"].layers[\"mindet_imputed\"]\n",
    "print(f\"Imputed values: {(mindet_matrix.M == 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Advanced Methods\n",
    "\n",
    "### 5.1 Local Least Squares (LLS) Imputation\n",
    "\n",
    "**Algorithm:** Combines KNN with local linear regression. For each sample with missing values, finds K nearest neighbors and builds a local linear model to predict missing values.\n",
    "\n",
    "**Best for:** High-dimensional data with correlated features.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `k`: Number of neighbors (default: 10)\n",
    "- `max_iter`: Maximum iterations for convergence\n",
    "- `tol`: Convergence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LLS imputation\n",
    "print(\"Running LLS imputation (this may take a moment)...\")\n",
    "\n",
    "container = impute_lls(\n",
    "    container,\n",
    "    assay_name=\"proteins\",\n",
    "    source_layer=\"log\",\n",
    "    new_layer_name=\"lls_imputed\",\n",
    "    k=10,\n",
    "    max_iter=10,  # Reduced for faster execution\n",
    ")\n",
    "\n",
    "print(\"LLS imputation completed.\")\n",
    "\n",
    "# Check results\n",
    "lls_matrix = container.assays[\"proteins\"].layers[\"lls_imputed\"]\n",
    "print(f\"Imputed values: {(lls_matrix.M == 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 MissForest (Random Forest) Imputation\n",
    "\n",
    "**Algorithm:** Iterative random forest imputation. For each feature with missing values, trains a random forest on observed features and predicts missing values.\n",
    "\n",
    "**Best for:** Complex non-linear patterns, mixed data types.\n",
    "\n",
    "**Note:** This is the slowest method but often most accurate for complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MissForest imputation\n",
    "print(\"Running MissForest imputation (this may take longer)...\")\n",
    "\n",
    "try:\n",
    "    container = impute_mf(\n",
    "        container,\n",
    "        assay_name=\"proteins\",\n",
    "        base_layer=\"log\",\n",
    "        new_layer_name=\"mf_imputed\",\n",
    "        max_iter=10,  # Maximum iterations\n",
    "        n_estimators=50,  # Number of trees (reduced for speed)\n",
    "    )\n",
    "    print(\"MissForest imputation completed.\")\n",
    "\n",
    "    # Check results\n",
    "    mf_matrix = container.assays[\"proteins\"].layers[\"mf_imputed\"]\n",
    "    print(f\"Imputed values: {(mf_matrix.M == 5).sum()}\")\n",
    "except ImportError:\n",
    "    print(\"MissForest skipped: scikit-learn not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Performance Comparison\n",
    "\n",
    "### 6.1 Visualize Imputation Results\n",
    "\n",
    "Let's compare the distributions of imputed values across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all imputed layers\n",
    "imputed_layers = {\n",
    "    \"KNN\": \"knn_imputed\",\n",
    "    \"PPCA\": \"ppca_imputed\",\n",
    "    \"BPCA\": \"bpca_imputed\",\n",
    "    \"SVD\": \"svd_imputed\",\n",
    "    \"LLS\": \"lls_imputed\",\n",
    "    \"QRILC\": \"qrilc_imputed\",\n",
    "    \"MinProb\": \"minprob_imputed\",\n",
    "    \"MinDet\": \"mindet_imputed\",\n",
    "}\n",
    "\n",
    "# Filter layers that exist\n",
    "available_layers = {\n",
    "    k: v for k, v in imputed_layers.items() if v in container.assays[\"proteins\"].layers\n",
    "}\n",
    "\n",
    "print(f\"Available imputed layers: {list(available_layers.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize imputation distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original log data for comparison\n",
    "log_data = container.assays[\"proteins\"].layers[\"log\"].X.flatten()[:5000]\n",
    "axes[0].hist(log_data, bins=50, color=\"gray\", alpha=0.7, edgecolor=\"black\")\n",
    "axes[0].set_title(\"Original (Log, with NaNs)\")\n",
    "axes[0].set_xlabel(\"Intensity\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Plot each imputed method\n",
    "for i, (name, layer_name) in enumerate(available_layers.items(), 1):\n",
    "    if i >= len(axes):\n",
    "        break\n",
    "    X = container.assays[\"proteins\"].layers[layer_name].X.flatten()[:5000]\n",
    "    axes[i].hist(X, bins=50, alpha=0.7, edgecolor=\"black\")\n",
    "    axes[i].set_title(f\"{name}\")\n",
    "    axes[i].set_xlabel(\"Intensity\")\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "    axes[i].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for i in range(len(available_layers) + 1, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/imputation_distributions.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Distribution comparison saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Pairwise Scatter Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise scatter comparison of methods\n",
    "methods_to_compare = list(available_layers.items())[:4]  # First 4 methods\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Select a subset of samples for visualization\n",
    "n_samples_show = min(500, container.n_samples)\n",
    "n_features_show = min(20, container.assays[\"proteins\"].n_features)\n",
    "\n",
    "for idx, (name1, layer1) in enumerate(methods_to_compare):\n",
    "    if idx >= len(methods_to_compare) - 1:\n",
    "        break\n",
    "\n",
    "    name2, layer2 = methods_to_compare[idx + 1]\n",
    "\n",
    "    X1 = container.assays[\"proteins\"].layers[layer1].X[:n_samples_show, :n_features_show].flatten()\n",
    "    X2 = container.assays[\"proteins\"].layers[layer2].X[:n_samples_show, :n_features_show].flatten()\n",
    "\n",
    "    axes[idx].scatter(X1, X2, alpha=0.3, s=5)\n",
    "    axes[idx].plot([X1.min(), X1.max()], [X1.min(), X1.max()], \"r--\", lw=2)\n",
    "    axes[idx].set_xlabel(f\"{name1}\")\n",
    "    axes[idx].set_ylabel(f\"{name2}\")\n",
    "    axes[idx].set_title(f\"{name1} vs {name2}\")\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "    # Compute correlation\n",
    "    corr = np.corrcoef(X1, X2)[0, 1]\n",
    "    axes[idx].text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        f\"r = {corr:.3f}\",\n",
    "        transform=axes[idx].transAxes,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5),\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/imputation_scatter_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Scatter comparison saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Statistical Summary of Imputation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for name, layer_name in available_layers.items():\n",
    "    X = container.assays[\"proteins\"].layers[layer_name].X\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Method\": name,\n",
    "            \"Mean\": np.mean(X),\n",
    "            \"Std\": np.std(X),\n",
    "            \"Min\": np.min(X),\n",
    "            \"Median\": np.median(X),\n",
    "            \"Max\": np.max(X),\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nImputation Summary Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(\"tutorial_output/imputation_summary.csv\", index=False)\n",
    "print(\"\\nSummary saved to tutorial_output/imputation_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Correlation Heatmap of Imputed Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap between methods\n",
    "method_names = list(available_layers.keys())\n",
    "n_methods = len(method_names)\n",
    "corr_matrix = np.zeros((n_methods, n_methods))\n",
    "\n",
    "# Compute correlations\n",
    "for i, name1 in enumerate(method_names):\n",
    "    for j, name2 in enumerate(method_names):\n",
    "        layer1 = available_layers[name1]\n",
    "        layer2 = available_layers[name2]\n",
    "        X1 = container.assays[\"proteins\"].layers[layer1].X.flatten()[:10000]\n",
    "        X2 = container.assays[\"proteins\"].layers[layer2].X.flatten()[:10000]\n",
    "        corr_matrix[i, j] = np.corrcoef(X1, X2)[0, 1]\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(corr_matrix, cmap=\"RdBu_r\", vmin=-1, vmax=1, aspect=\"auto\")\n",
    "\n",
    "# Add labels\n",
    "ax.set_xticks(range(n_methods))\n",
    "ax.set_yticks(range(n_methods))\n",
    "ax.set_xticklabels(method_names, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(method_names)\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(n_methods):\n",
    "    for j in range(n_methods):\n",
    "        text = ax.text(\n",
    "            j, i, f\"{corr_matrix[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\", fontsize=9\n",
    "        )\n",
    "\n",
    "ax.set_title(\"Correlation Between Imputation Methods\")\n",
    "plt.colorbar(im, ax=ax, label=\"Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tutorial_output/imputation_correlation_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation heatmap saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Best Practices\n",
    "\n",
    "### 7.1 Imputation Workflow\n",
    "\n",
    "```python\n",
    "# Recommended workflow\n",
    "from scptensor import (\n",
    "    load_csv, norm_log, count_mask_codes,\n",
    "    impute_knn, impute_qrilc\n",
    ")\n",
    "\n",
    "# 1. Load data\n",
    "container = load_csv(\"data.csv\")\n",
    "\n",
    "# 2. Analyze missingness patterns\n",
    "mask_counts = count_mask_codes(container.assays['proteins'].layers['raw'].M)\n",
    "lod_ratio = mask_counts.get(2, 0) / sum(mask_counts.values())\n",
    "\n",
    "# 3. Normalize\n",
    "container = norm_log(container, \"proteins\", \"raw\", \"log\")\n",
    "\n",
    "# 4. Choose imputation method based on missingness\n",
    "if lod_ratio > 0.5:  # Mostly MNAR\n",
    "    container = impute_qrilc(container, \"proteins\", \"log\", \"imputed\")\n",
    "else:  # Mostly MCAR\n",
    "    container = impute_knn(container, \"proteins\", \"log\", \"imputed\", k=10)\n",
    "\n",
    "# 5. Verify imputation\n",
    "imputed_matrix = container.assays['proteins'].layers['imputed']\n",
    "print(f\"Imputed: {(imputed_matrix.M == 5).sum()} values\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Choosing the Right Method\n",
    "\n",
    "| Scenario | Recommended Method | Alternative |\n",
    "|----------|-------------------|-------------|\n",
    "| Quick analysis, small data | KNN | PPCA |\n",
    "| Large dataset (>10k samples) | PPCA | SVD |\n",
    "| High LOD missingness | QRILC | MinProb |\n",
    "| Correlated features | LLS | BPCA |\n",
    "| Complex patterns, enough time | MissForest | BPCA |\n",
    "| Baseline comparison | MinDet | KNN |\n",
    "| Automatic model selection | BPCA | PPCA |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Common Pitfalls to Avoid\n",
    "\n",
    "1. **Not analyzing missingness type first**\n",
    "   - Always check if missingness is MCAR or MNAR\n",
    "   - Use `count_mask_codes()` to understand mask distribution\n",
    "\n",
    "2. **Applying imputation on raw, unnormalized data**\n",
    "   - Most methods work better on log-transformed data\n",
    "   - Normalize first, then impute\n",
    "\n",
    "3. **Using MNAR methods for MCAR data**\n",
    "   - QRILC/MinProb will underestimate values for MCAR missingness\n",
    "   - KNN/PPCA are better for random missingness\n",
    "\n",
    "4. **Ignoring imputation uncertainty**\n",
    "   - Single imputation doesn't capture uncertainty\n",
    "   - Consider multiple imputations for critical analyses\n",
    "\n",
    "5. **Over-imputing**\n",
    "   - Features with >80% missingness may be better removed\n",
    "   - Use `filter_features_missing()` before imputation\n",
    "\n",
    "6. **Not validating results**\n",
    "   - Always check imputed value distributions\n",
    "   - Compare with original data visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Parameter Tuning Guide\n",
    "\n",
    "**KNN:**\n",
    "- `k=5-15`: Typical range\n",
    "- Smaller k for heterogeneous data\n",
    "- Larger k for homogeneous data\n",
    "\n",
    "**PPCA/BPCA:**\n",
    "- `n_components`: Start with 5-20\n",
    "- Use elbow method on scree plot\n",
    "- BPCA automatically prunes unnecessary components\n",
    "\n",
    "**QRILC:**\n",
    "- `q=0.01-0.05`: Recommended for proteomics\n",
    "- Lower q for more aggressive censoring\n",
    "\n",
    "**MinProb:**\n",
    "- `sigma=1.0-3.0`: Typical range\n",
    "- Higher sigma for more variance in imputed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Summary\n",
    "\n",
    "**All 10 Imputation Methods in ScpTensor:**\n",
    "\n",
    "| # | Method | Category | Key Feature | Use When... |\n",
    "|---|--------|----------|-------------|-------------|\n",
    "| 1 | `impute_knn` | MCAR | Fast, simple | Small datasets, general use |\n",
    "| 2 | `impute_ppca` | MCAR | Scalable | Large datasets |\n",
    "| 3 | `impute_bpca` | MCAR | Auto model selection | Uncertain about components |\n",
    "| 4 | `impute_svd` | MCAR | Dimensionality reduction | Need fast imputation |\n",
    "| 5 | `impute_mf` | MCAR/MAR | Non-linear | Complex patterns, time available |\n",
    "| 6 | `impute_lls` | MCAR | Exploits correlation | Correlated features |\n",
    "| 7 | `impute_qrilc` | MNAR | Best for LOD | Below detection limit |\n",
    "| 8 | `impute_minprob` | MNAR | Fast MNAR | MNAR, need speed |\n",
    "| 9 | `impute_mindet` | MNAR | Deterministic | Quick baseline |\n",
    "| 10 | `impute_nmf` | MCAR | Non-negative | Count-like data |\n",
    "\n",
    "**Quick Decision Guide:**\n",
    "- **Start with KNN** for general-purpose imputation\n",
    "- **Use QRILC** when most missingness is LOD-related\n",
    "- **Try BPCA** for automatic model selection\n",
    "- **Use MissForest** for the best accuracy (if time allows)\n",
    "\n",
    "**Next Tutorials:**\n",
    "- Tutorial 04: Clustering and Visualization\n",
    "- Tutorial 07: Feature Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
