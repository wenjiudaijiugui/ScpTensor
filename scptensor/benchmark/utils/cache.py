"""Cache management for benchmark results.

This module provides a caching system for benchmark results to enable
incremental benchmark runs. Results are cached based on module name,
dataset name, and parameter combination using MD5 hash keys.
"""

from __future__ import annotations

import hashlib
import json
import pickle
from pathlib import Path
from typing import Any

from scptensor.benchmark.modules.base import ModuleResult

__all__ = ["CacheManager"]


class CacheManager:
    """Manager for caching benchmark results.

    CacheManager stores and retrieves benchmark results based on a unique
    key derived from module name, dataset name, and parameters. This enables
    incremental benchmark runs where only unchanged configurations are
    re-computed.

    Parameters
    ----------
    cache_dir : Path | str, default="benchmark_results/cache"
        Directory path for storing cache files. Created if it does not exist.

    Attributes
    ----------
    cache_dir : Path
        Directory where cache files are stored.

    Examples
    --------
    >>> from scptensor.benchmark.utils.cache import CacheManager
    >>> from scptensor.benchmark.modules.base import ModuleResult
    >>>
    >>> cache = CacheManager("my_cache_dir")
    >>> results = [ModuleResult(...)]
    >>>
    >>> # Store results
    >>> cache.set("normalization", "dataset1", {"method": "log"}, results)
    >>>
    >>> # Retrieve results
    >>> cached = cache.get("normalization", "dataset1", {"method": "log"})
    >>> assert cached is not None
    >>>
    >>> # Clear all cache for a module
    >>> cache.clear("normalization")
    """

    def __init__(self, cache_dir: Path | str = Path("benchmark_results/cache")) -> None:
        """Initialize the cache manager.

        Parameters
        ----------
        cache_dir : Path | str, default="benchmark_results/cache"
            Directory path for storing cache files. Created if it does not exist.
        """
        self._cache_dir: Path = Path(cache_dir)
        self._cache_dir.mkdir(parents=True, exist_ok=True)

    @property
    def cache_dir(self) -> Path:
        """Get the cache directory path.

        Returns
        -------
        Path
            Directory where cache files are stored.
        """
        return self._cache_dir

    def get_cache_key(self, module: str, dataset: str, params: dict[str, Any]) -> str:
        """Generate a unique cache key for the given parameters.

        The key is generated by creating an MD5 hash of the concatenated
        module name, dataset name, and sorted parameters. This ensures that
        the same parameters always produce the same cache key regardless
        of parameter ordering.

        Parameters
        ----------
        module : str
            Name of the benchmark module.
        dataset : str
            Name of the dataset.
        params : dict[str, Any]
            Dictionary of parameters for the benchmark run.

        Returns
        -------
        str
            MD5 hash string representing the unique cache key.

        Examples
        --------
        >>> cache = CacheManager()
        >>> key = cache.get_cache_key("normalization", "data", {"method": "log"})
        >>> assert len(key) == 32  # MD5 hash length
        """
        # Sort parameters for consistent hashing
        sorted_params = json.dumps(params, sort_keys=True, default=str)

        # Create hash string from module + dataset + params
        hash_input = f"{module}:{dataset}:{sorted_params}"
        return hashlib.md5(hash_input.encode()).hexdigest()

    def get(
        self, module: str, dataset: str, params: dict[str, Any]
    ) -> list[ModuleResult] | None:
        """Retrieve cached results for the given parameters.

        Attempts to load previously cached results. Returns None if no
        cache exists for the given key.

        Parameters
        ----------
        module : str
            Name of the benchmark module.
        dataset : str
            Name of the dataset.
        params : dict[str, Any]
            Dictionary of parameters for the benchmark run.

        Returns
        -------
        list[ModuleResult] | None
            Cached results if found, None otherwise.

        Examples
        --------
        >>> cache = CacheManager()
        >>> results = cache.get("normalization", "data", {"method": "log"})
        >>> if results:
        ...     print(f"Found {len(results)} cached results")
        """
        cache_key = self.get_cache_key(module, dataset, params)
        cache_file = self._cache_dir / f"{cache_key}.pkl"

        if not cache_file.exists():
            return None

        try:
            with cache_file.open("rb") as f:
                return pickle.load(f)
        except (OSError, pickle.PickleError):
            # Corrupted cache file - treat as cache miss
            return None

    def set(
        self,
        module: str,
        dataset: str,
        params: dict[str, Any],
        results: list[ModuleResult],
    ) -> None:
        """Store benchmark results in cache.

        Caches the provided results using a key derived from the parameters.
        Existing cache for the same key is overwritten.

        Parameters
        ----------
        module : str
            Name of the benchmark module.
        dataset : str
            Name of the dataset.
        params : dict[str, Any]
            Dictionary of parameters for the benchmark run.
        results : list[ModuleResult]
            Results to cache.

        Examples
        --------
        >>> from scptensor.benchmark.modules.base import ModuleResult
        >>> cache = CacheManager()
        >>> results = [ModuleResult(
        ...     module_name="norm",
        ...     dataset_name="data",
        ...     method_name="log"
        ... )]
        >>> cache.set("normalization", "data", {"method": "log"}, results)
        """
        cache_key = self.get_cache_key(module, dataset, params)
        cache_file = self._cache_dir / f"{cache_key}.pkl"

        with cache_file.open("wb") as f:
            pickle.dump(results, f)

    def clear(self, module: str | None = None) -> None:
        """Clear cached results.

        If a module name is provided, only clears cache entries for that
        module. Otherwise, clears all cached results.

        Parameters
        ----------
        module : str | None, default=None
            Name of the module to clear cache for. If None, clears all cache.

        Examples
        --------
        >>> cache = CacheManager()
        >>> cache.clear("normalization")  # Clear specific module
        >>> cache.clear()  # Clear all cache
        """
        if module is None:
            # Clear all cache files
            for cache_file in self._cache_dir.glob("*.pkl"):
                cache_file.unlink(missing_ok=True)
        else:
            # Clear only cache files for the specified module
            # Since we don't store module name in the filename,
            # we need to read all files and check their contents
            for cache_file in self._cache_dir.glob("*.pkl"):
                try:
                    with cache_file.open("rb") as f:
                        results: list[ModuleResult] = pickle.load(f)
                        if results and results[0].module_name == module:
                            cache_file.unlink(missing_ok=True)
                except (OSError, pickle.PickleError, IndexError):
                    # Skip corrupted or empty cache files
                    continue

    def is_valid(self, module: str, dataset: str, params: dict[str, Any]) -> bool:
        """Check if valid cached results exist for the given parameters.

        Parameters
        ----------
        module : str
            Name of the benchmark module.
        dataset : str
            Name of the dataset.
        params : dict[str, Any]
            Dictionary of parameters for the benchmark run.

        Returns
        -------
        bool
            True if valid cache exists, False otherwise.

        Examples
        --------
        >>> cache = CacheManager()
        >>> if cache.is_valid("normalization", "data", {"method": "log"}):
        ...     print("Using cached results")
        ... else:
        ...     print("Running benchmark...")
        """
        return self.get(module, dataset, params) is not None

    def get_cache_info(self) -> dict[str, Any]:
        """Get information about the current cache state.

        Returns
        -------
        dict[str, Any]
            Dictionary containing cache statistics including total files,
            total size in bytes, and module breakdown.

        Examples
        --------
        >>> cache = CacheManager()
        >>> info = cache.get_cache_info()
        >>> print(f"Cache size: {info['total_size_mb']:.2f} MB")
        """
        cache_files = list(self._cache_dir.glob("*.pkl"))
        total_size = sum(f.stat().st_size for f in cache_files)

        # Count by module
        module_counts: dict[str, int] = {}
        for cache_file in cache_files:
            try:
                with cache_file.open("rb") as f:
                    results: list[ModuleResult] = pickle.load(f)
                    if results:
                        module_name = results[0].module_name
                        module_counts[module_name] = module_counts.get(module_name, 0) + 1
            except (OSError, pickle.PickleError, IndexError):
                continue

        return {
            "cache_dir": str(self._cache_dir),
            "total_files": len(cache_files),
            "total_size_bytes": total_size,
            "total_size_mb": total_size / (1024 * 1024),
            "module_counts": module_counts,
        }

    def prune_by_size(self, max_size_mb: float) -> int:
        """Prune cache files to keep total size under the limit.

        Removes oldest cache files (by modification time) until total
        cache size is below the specified limit.

        Parameters
        ----------
        max_size_mb : float
            Maximum cache size in megabytes.

        Returns
        -------
        int
            Number of cache files removed.

        Examples
        --------
        >>> cache = CacheManager()
        >>> removed = cache.prune_by_size(100)  # Limit to 100 MB
        >>> print(f"Removed {removed} cache files")
        """
        cache_files = list(self._cache_dir.glob("*.pkl"))

        # Calculate current size
        total_size = sum(f.stat().st_size for f in cache_files)
        max_size_bytes = max_size_mb * 1024 * 1024

        if total_size <= max_size_bytes:
            return 0

        # Sort by modification time (oldest first)
        cache_files.sort(key=lambda f: f.stat().st_mtime)

        # Remove oldest files until under limit
        removed_count = 0
        for cache_file in cache_files:
            if total_size <= max_size_bytes:
                break
            file_size = cache_file.stat().st_size
            cache_file.unlink(missing_ok=True)
            total_size -= file_size
            removed_count += 1

        return removed_count
